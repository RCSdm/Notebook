## Attention is All You Need 核心知识

### 1. Transformer 架构
- **完全基于注意力机制**：Transformer 摒弃传统的循环神经网络（RNN）和卷积神经网络（CNN），只使用注意力机制实现序列建模。
- **编码器-解码器结构**：整个模型由堆叠的编码器和解码器组成，每个部分都由若干相同结构的层构成。

### 2. 自注意力机制 (Self-Attention)
- **核心思想**：允许序列中每个位置直接关注其他所有位置，从而捕捉序列内部的全局依赖关系。
- **计算方式**：对查询 (Query)、键 (Key)、值 (Value) 进行线性变换，并通过 scaled dot-product 计算注意力权重：
  
  \[
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
  \]

  其中，\(d_k\) 为键的维度，用于缩放防止梯度消失。

### 3. 多头注意力 (Multi-Head Attention)
- **并行注意力**：将输入分别投影到多个子空间 (heads) 中，每个 head 独立实现自注意力机制。
- **优势**：使模型能从不同表示子空间中并行捕捉不同的特征信息，增强模型的表达能力。
- **合成输出**：将各 head 的输出拼接，再通过线性变换得到最终结果。

### 4. 位置编码 (Positional Encoding)
- **必要性**：Transformer 没有递归结构，无法直接捕捉序列的顺序信息，因此引入位置编码以提供位置信息。
- **实现方法**：常用正弦和余弦函数生成不同频率的周期性信号：
 
  \[
  PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right), \quad
  PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
  \]

  其中，\(pos\) 表示位置，\(i\) 表示维度索引。

### 5. 残差连接与层归一化 (Residual Connection & Layer Normalization)
- **残差连接**：在每个子层（如多头注意力和前馈网络）的输入上加上它的输出，有助于缓解梯度消失问题，支持深层网络训练。
- **层归一化**：在残差连接之后对数据进行归一化，稳定训练过程并加速收敛。

### 6. 前馈神经网络 (Position-wise Feed Forward Network)
- **结构**：每一层中包含一个全连接的前馈神经网络，对每个位置的表示进行独立的非线性变换，一般由两个线性层和一个激活函数（通常为 ReLU）构成。
- **目 的**：引入非线性变换进一步丰富表示能力。

### 7. 关键优势
- **并行计算**：由于不依赖递归结构，Transformer 支持更高效的并行计算。
- **全局上下文建模**：自注意力机制使每个位置能够直接捕捉全局信息，适用于长距离依赖任务。
- **扩展性强**：简化的架构便于扩展和堆叠，实现大规模预训练语言模型及其它应用。

### 8. 总结
Transformer 的设计理念和核心技术构成了现代自然语言处理和序列建模的基础，不仅在机器翻译任务中取得了显著进展，也为后续的模型（如 BERT、GPT 等）奠定了理论和实践基础。