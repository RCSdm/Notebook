## 扩散模型

扩散模型：和其他生成模型一样，实现从噪声（采样自简单的分布）生成目标数据样本。

扩散模型包括两个过程：前向过程（forward process）和反向过程（reverse process），其中前向过程又称为扩散过程（diffusion process）。无论是前向过程还是反向过程都是一个参数化的马尔可夫链（Markov chain），其中反向过程可用于生成数据样本（它的作用类似GAN中的生成器，只不过GAN生成器会有维度变化，而DDPM的反向过程没有维度变化）。

![alt text](<机器人交叉创新/截屏2025-04-21 15.54.45.png>)



### 1. 正向扩散过程（Forward Process）
将原始数据 $x_0$ 逐步添加高斯噪声，经过 $T$ 步后转化为纯噪声 $x_T$。  
**数学描述**（离散形式）：
$$
x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1-\alpha_t} \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, \mathbf{I})
$$
其中：
- $\alpha_t$ 是噪声调度系数（$\alpha_t \in (0,1)$）
- $t \in \{1,2,...,T\}$ 为时间步

**连续形式**（随机微分方程）：
$$
dx = f(x,t)dt + g(t)dw
$$
$w$ 为维纳过程，$f(\cdot)$ 和 $g(\cdot)$ 是预定义函数。

---

### 2. 逆向生成过程（Reverse Process）
通过神经网络学习去噪步骤，从 $x_T$ 逐步恢复数据 $x_0$。

**目标函数**（噪声预测）：
$$
\mathcal{L} = \mathbb{E}_{t,x_0,\epsilon}\left[ \|\epsilon - \epsilon_\theta(x_t,t)\|^2 \right]
$$
其中 $\epsilon_\theta$ 是神经网络（如U-Net）预测的噪声。

**采样步骤**：
$$
x_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left( x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t,t) \right) + \sigma_t z
$$
其中：
- $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$
- $z \sim \mathcal{N}(0,\mathbf{I})$
- $\sigma_t$ 为噪声系数

---


变分下界（ELBO）
$$
\log p(x) \geq \mathbb{E}_q \left[ \log \frac{p(x_{0:T})}{q(x_{1:T}|x_0)} \right]
$$

扩散核（Diffusion Kernel）
$$
q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)\mathbf{I})
$$

---



## 随机森林

将多个决策树结合在一起，每次数据集是随机有放回的选出，同时随机选出部分特征作为输入，所以该算法被称为随机森林算法。可以看到随机森林算法是以决策树为估计器的Bagging算法。

![alt text](机器人交叉创新/image.png)

随机森林算法的优点：

    1 对于很多种资料，可以产生高准确度的分类器
    2 可以处理大量的输入变量
    3 可以在决定类别时，评估变量的重要性
    4 在建造森林时，可以在内部对于一般化后的误差产生不偏差的估计
    5 包含一个好方法可以估计丢失的资料，并且如果有很大一部分的资料丢失，仍可以维持准确度
    6 对于不平衡的分类资料集来说，可以平衡误差
    7 可被延伸应用在未标记的资料上，这类资料通常是使用非监督式聚类，也可侦测偏离者和观看资料
    8 学习过程很快速
    
随机森林算法的缺点：

    1 牺牲了决策树的可解释性
    2 在某些噪音较大的分类或回归问题上会过拟合
    3 在多个分类变量的问题中，随机森林可能无法提高基学习器的准确性

## 模仿学习

模仿学习是一种在没有显式奖励信号的情况下，让智能体学习专家策略的方法。其核心思想是：通过观察专家的行为，学习一个策略，使其能够像专家一样完成任务。

模仿学习与强化学习有着密切的联系，但也有着本质的区别。强化学习的目标是优化一个由奖励函数定义的策略，而模仿学习的目标是复制一个由专家演示定义的策略。换句话说，强化学习是「创造」策略，而模仿学习是「模仿」策略。

### 行为克隆
行为克隆（Behavioral Cloning, BC） 是最简单、最直接的模仿学习方法。它的核心思想是将模仿学习视为一个监督学习问题，即：给定一系列专家演示数据，学习一个从状态到动作的映射。

假设我们有一组专家演示数据 \( D = \{ (s_i, a_i) \}_{i=1}^N \)，其中 \( s_i \) 表示状态，\( a_i \) 表示专家在状态 \( s_i \) 下采取的动作。行为克隆的目标是学习一个策略 \( \pi(a|s) \)，使得在给定状态 \( s \) 的情况下，策略 \( \pi(a|s) \) 能够预测出与专家动作 \( a \) 相似的动作。

形式化地说，行为克隆的目标是最小化以下损失函数：

$$
\mathcal{L}(\pi) = \mathbb{E}_{(s,a) \sim D} \left[ l(\pi(a|s), a) \right]
$$

其中，\( l(\cdot, \cdot) \) 表示一个合适的损失函数，例如交叉熵损失（Cross-Entropy Loss）或均方误差损失（Mean Squared Error Loss）。

### 逆强化学习
逆强化学习（Inverse Reinforcement Learning, IRL）是另一种重要的模仿学习方法。与行为克隆不同，逆强化学习不是直接学习策略，而是从专家演示中推断奖励函数，然后使用强化学习方法优化策略。

逆强化学习的思想是：假设专家之所以能够做出好的行为，是因为他们优化了一个未知的奖励函数。因此，我们可以从专家演示中学习这个未知的奖励函数，然后使用强化学习方法优化策略。

形式化地说，逆强化学习的目标是找到一个奖励函数 $R(s, a)$，使得专家策略 $\pi_E$ 在该奖励函数下的期望累积回报最大：

$$
R^* = \arg\max_R \mathbb{E}_{\pi_E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \right]
$$

其中，$\gamma$ 是折扣因子，$\pi_E$ 表示专家策略。

### 生成对抗模仿学习
生成对抗模仿学习（Generative Adversarial Imitation Learning, GAIL）是一种基于生成对抗网络（GAN）的模仿学习方法。它通过对抗训练，学习一个能够模仿专家行为的智能体。

对抗模仿学习的核心思想是：将模仿学习问题转化为一个生成对抗问题。具体来说，我们训练一个智能体（生成器）和一个判别器，智能体试图模仿专家行为，判别器试图区分智能体和专家的轨迹。通过对抗训练，智能体逐渐学习到接近专家策略的行为。

形式化地说，对抗模仿学习的目标是找到一个策略 \(\pi\) 和一个判别器 \(D\)，使得：

$$
\min_{\pi} \max_{D} \mathbb{E}_{(s, a) \sim \pi_E} [\log D(s, a)] + \mathbb{E}_{(s, a) \sim \pi} [\log(1 - D(s, a))]
$$

其中，\(\pi_E\) 表示专家策略，\(\pi\) 表示智能体策略，\(D(s, a)\) 表示判别器对状态-动作对 \((s, a)\) 的判别结果，取值范围为 \([0, 1]\)。

GAIL 除了可以克服奖励函数多义性问题外，还可以学习专家策略中的复杂行为模式，例如多模态行为（Multi-modal Behavior）。但 GAN 也是出了名的难以训练，容易出现梯度消失、模式崩塌等问题。

## 对抗学习
GAN的全称是Generative adversarial network：
生成网络(Generator)负责生成模拟数据；判别网络(Discriminator)负责判断输入的数据是真实的还是生成的。生成网络要不断优化自己生成的数据让判别网络判断不出来，判别网络也要优化自己让自己判断得更准确。二者关系形成对抗，因此叫对抗网络。

![alt text](<机器人交叉创新/截屏2025-04-21 15.39.16.png>)

## Planning with Diffusion for Flexible Behavior Synthesis

基于模型的强化学习算法通常仅利用学习来估计近似的动态模型，将其余的决策工作交给经典的轨迹优化器。尽管这种组合在概念上很简单，但经验上存在一些缺陷，表明学习到的模型可能不太适合标准的轨迹优化。在本文中，我们考虑将轨迹优化步骤融入到建模过程中，这样从模型中采样与从模型中规划轨迹几乎是相同的任务。我们的技术方法的核心是一种扩散概率模型，它通过迭代去噪轨迹来进行规划。我们将分类器引导采样和图像修补重新解释为连贯的规划策略，探索了基于扩散的规划算法独特且有用的性质，并展示了我们的框架在强调长期决策和测试目标适应性的控制环境中的有效性。

传统的基于模型的规划算法都是自回归地向前预测，而 Diffuser 能够同时预测路径规划中所有的时间步。扩散模型的迭代采样过程具有灵活的添加条件的能力，允许辅助引导信息修改采样的过程，以还原出高收益或满足一系列约束条件的轨迹。这种基于数据驱动的轨迹优化算法具有几种迷人的特点：

* 长程可扩展性 Diffuser 是以生成轨迹的准确性——而非单步转移的准确性——为目标进行训练的，因此它不会受到单步动力学模型在长时间的序列上，推演误差会逐步累积放大的影响，能够更好地应对长时间的规划需求。
* 任务可组合性 奖励函数提供了采样规划时可用的辅助梯度。通过将多个奖励梯度相加，我们可以简单地将它们组合起来以进行规划。
* 时序可组合性 Diffuser 通过迭代地提高局部一致性来生成全局一致的轨迹，这也就意味着它可以通过将训练轨迹分布中的转移子序列拼接在一起来推广得到分布以外的新的轨迹。
* 高效的非贪心规划 通过模糊模型和规划器之间的边界，用于改进模型预测能力的训练过程也会提高其规划能力。这种设计产生了一个经过学习得来的规划器，可以解决许多传统规划算法难以解决的长时程、稀疏奖励的问题。

本工作的核心贡献是为轨迹数据设计了一种去噪扩散模型，并提供了一个行为合成的概率框架。



diffusion model 具有更强的建模复杂分布的能力，能够更好地建模表征数据的各种特性，但是决策类问题除了需要处理表征建模的挑战，更注重于学习智能体策略的有效性和多样性。一种可行的思路是选择模仿学习方法，使用扩散模型来更好地“模仿”，“拟合”专家策略的数据分布

## Motion Planning Diffusion:  Learning and Planning of Robot Motions with Diffusion Models


Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multimodality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several baselines in simulated planar robot and 7-dof robot arm manipulator environments. To assess the generalization capabilities of our method, we test it in environments with previously unseen obstacles. Our experiments show that diffusion models are strong priors to encode high-dimensional trajectory distributions of robot motions.

![alt text](<机器人交叉创新/截屏2025-04-24 11.07.25.png>)

根据提供的两篇论文，扩散模型（DDPMs），特别是应用于运动规划的Motion Planning Diffusion (MPD) 和 Diffuser，展现出以下主要优势：

## DDPM 的主要优势总结 🌟

1.  **强大的分布建模能力：** 扩散模型能够有效地编码复杂、高维数据分布 [cite: 1, 7]，尤其擅长捕捉数据中的多模态特性 [cite: 7, 30]。这对于机器人运动规划中可能存在多种有效路径的情况尤为重要 [cite: 7, 24]。
2.  **高质量的样本生成：** 相比于一些其他生成模型（如CVAE），扩散模型在生成样本时具有更优的性能 [cite: 30]，能够生成更平滑、更多样化且更可能无碰撞的轨迹 [cite: 32, 184, 185]。
3.  **灵活的条件引导：** 扩散模型的去噪过程允许引入外部信息（如任务目标、约束）来引导采样过程 [cite: 6, 255, 280]，从而生成满足特定需求的轨迹。MPD通过结合代价梯度的引导实现这一点 [cite: 6, 98]，而Diffuser则通过分类器引导或图像修复（inpainting）的方式实现 [cite: 255, 309]。
4.  **端到端的轨迹优化：** 扩散模型可以直接生成整个轨迹，并在生成过程中进行优化 [cite: 29, 253]，而不是分步预测或依赖后续优化器。这使得模型训练和规划过程更紧密地结合 [cite: 253, 266]。
5.  **作为优秀的先验：** 学习到的扩散模型可以作为强大的先验知识 [cite: 5, 10]，用于加速或改进传统的运动规划算法 [cite: 2]，例如为基于优化的规划器（如GPMP）提供高质量的初始化 [cite: 148, 195]。

---
## 详细概念解读 🧐

以下是对您提出的几个关键特性的详细解读，主要基于 "Planning with Diffusion for Flexible Behavior Synthesis" (Diffuser) 论文，并结合 "Motion Planning Diffusion" (MPD) 的相关概念：

### 长程可扩展性 (Long-horizon scalability)

**长程可扩展性** 指的是模型在处理和规划需要长时间跨度、包含许多步骤的序列（如复杂任务的轨迹）时，其性能不会显著下降，并且能够有效避免错误累积的能力。

* **Diffuser 的阐述：** Diffuser 模型训练的目标是其生成轨迹的准确性，而非单步预测的误差 [cite: 281]。这使得它不像传统的单步动态模型那样会遭受随时间累积的 rollout 误差，因此在长规划时域上能更优雅地扩展 [cite: 281]。其有效的长程预测能力直接转化为有效的长程规划能力 [cite: 361]。
* **MPD 的体现：** 虽然 MPD 论文没有直接使用“长程可扩展性”这一术语，但它通过学习整个轨迹的分布 [cite: 207]，并在去噪的每一步都考虑全局成本 [cite: 98]，避免了传统优化方法容易陷入局部最优或短视的问题，从而有能力处理较长的轨迹规划。

---
### 任务可组合性 (Task compositionality)

**任务可组合性** 指的是模型能够灵活地组合或适应不同任务需求的能力，通常无需为每个新任务或任务组合重新训练整个模型。

* **Diffuser 的阐述：** Diffuser 的核心模型（扩散模型本身）是独立于奖励函数的 [cite: 271, 369]。规划可以通过相对轻量级的扰动函数 $h(\tau)$ (perturbation functions) 来引导 [cite: 370]，这些函数可以对应不同的奖励或成本，甚至可以组合多个扰动函数以同时满足多个目标 [cite: 370]。例如，可以通过叠加不同奖励函数的梯度来组合多个奖励 [cite: 282]。
* **MPD 的体现：** MPD 的规划过程允许结合多个运动规划成本项（如碰撞、平滑度、目标到达）[cite: 73]，通过加权梯度 $g = -\sum_{i}\lambda_{i}\nabla_{\tau_{t-1}}c_{i}(\tau_{t-1}=\mu_{t})$ 来引导轨迹生成 [cite: 94, 96]。这种方式可以看作是一种任务组合，其中不同的成本项代表了任务的不同方面或子目标。

---
### 时序可组合性 (Temporal compositionality)

**时序可组合性** 描述了模型通过组合或“拼接”已学习到的行为片段（子序列）来生成新的、可能未在训练数据中直接见过的连贯长序列的能力。

* **Diffuser 的阐述：** Diffuser 通过迭代地改进轨迹的局部一致性来生成全局连贯的轨迹 [cite: 283, 318]，允许它通过将熟悉的子序列以新的方式拼接在一起来泛化到新的轨迹 [cite: 283, 365]。尽管它不是一个马尔可夫模型，但它的每个去噪步骤都基于局部信息进行预测，多步组合后能确保全局的连贯性 [cite: 276, 318]。
* **MPD 的体现：** MPD 中的扩散模型通过在整个轨迹上操作的U-Net架构进行学习 [cite: 89, 38]。虽然没有明确论述“拼接”片段，但U-Net结构本身具有捕捉不同尺度特征的能力，其去噪过程也是逐步从噪声中恢复结构，这暗示了其可以学习和组合轨迹中的时序模式。

---
### 高效非贪心规划 (Effective non-greedy planning)

**高效非贪心规划** 指的是规划方法不局限于每一步的局部最优选择（贪心策略），而是能够进行更长远的考虑，以找到全局更优的解决方案，尤其是在面对稀疏奖励或需要复杂序列决策的问题时。

* **Diffuser 的阐述：** 通过模糊模型和规划器之间的界限，Diffuser 的训练过程在改进模型预测的同时也提升了其规划能力 [cite: 272]。这产生了一个学习到的规划器，能够解决传统规划方法难以处理的长程、稀疏奖励问题 [cite: 273, 362]。Diffuser 的学习规划过程不会遭受常见于 shooting 算法的短视失败模式 [cite: 348, 362]。
* **MPD 的体现：** MPD 将运动规划表述为“规划即推理”(planning-as-inference) [cite: 34, 78]，目标是采样整个轨迹的后验分布 $p(\tau|\mathcal{O})$ [cite: 78]。在每个去噪步骤中，它都会考虑整个轨迹的成本梯度来引导样本向低成本区域移动 [cite: 94, 98]，这本质上是一种非贪心的全局优化视角。与首先从先验采样然后进行优化的方法相比，MPD将先验采样和运动优化合并到一个算法中 [cite: 29]。